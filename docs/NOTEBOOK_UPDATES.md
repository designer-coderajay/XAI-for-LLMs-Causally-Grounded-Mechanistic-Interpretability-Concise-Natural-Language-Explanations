# üìù NOTEBOOK UPDATE INSTRUCTIONS

## How to Add Reproducibility to All Notebooks

---

## üîß UNIVERSAL CELL 2 UPDATE

Add this code block at the START of Cell 2 in EVERY notebook (01-10):

```python
# ==============================================================================
# REPRODUCIBILITY SETUP
# ==============================================================================

import torch
import numpy as np
import random
import sys
from datetime import datetime

# Set seeds for reproducibility
SEED = 42
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
np.random.seed(SEED)
random.seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

print("=" * 60)
print("ENVIRONMENT & REPRODUCIBILITY")
print("=" * 60)
print(f"Random Seed:     {SEED}")
print(f"Python:          {sys.version.split()[0]}")
print(f"PyTorch:         {torch.__version__}")
print(f"CUDA Available:  {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA Version:    {torch.version.cuda}")
    print(f"GPU:             {torch.cuda.get_device_name(0)}")
try:
    import transformer_lens
    print(f"TransformerLens: {transformer_lens.__version__}")
except:
    pass
print(f"Timestamp:       {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 60)
```

---

## üìì NOTEBOOK-SPECIFIC UPDATES

### Notebook 01: Setup Test

**Header update:**
```python
"""
================================================================================
NOTEBOOK 01: ENVIRONMENT SETUP & VERIFICATION
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Verify all dependencies are installed and GPU is available.
Sample Size: N/A (setup only)
================================================================================
"""
```

---

### Notebook 02: IOI Reproduction

**Header update:**
```python
"""
================================================================================
NOTEBOOK 02: IOI CIRCUIT REPRODUCTION
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Reproduce the Indirect Object Identification circuit from 
         Wang et al. (2022) "Interpretability in the Wild"

Sample Size: n=50 prompts (25 name pairs √ó 2 templates)
Key Reference: Wang et al., 2022 (arXiv:2211.00593)
================================================================================
"""
```

---

### Notebook 03: NL Explanation Generator

**Header update:**
```python
"""
================================================================================
NOTEBOOK 03: RULE-BASED TEMPLATE EXPLANATION BASELINE
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Generate template-based natural language explanations as BASELINE.
         These are deterministic, rule-based mappings (NOT learned).

Note: This is the BASELINE method. See Notebook 08 for learned (LLM) method.
================================================================================
"""
```

---

### Notebook 04: Baselines Comparison

**Header update:**
```python
"""
================================================================================
NOTEBOOK 04: BASELINE COMPARISON (PRELIMINARY)
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Compare our circuit-based method against attention and random baselines.

Sample Size: n=5 (PRELIMINARY - see Notebook 05 for full n=50 evaluation)
Key Reference: Jain & Wallace, 2019 (Attention is Not Explanation)
================================================================================
"""
```

---

### Notebook 05: Expanded Evaluation (CANONICAL)

**Header update:**
```python
"""
================================================================================
NOTEBOOK 05: FULL ERASER EVALUATION (CANONICAL RESULTS)
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Complete ERASER faithfulness evaluation on full dataset.
         THIS IS THE CANONICAL EVALUATION - cite these results in thesis.

Sample Size: n=50 prompts (CANONICAL)
Key Reference: DeYoung et al., 2020 (ERASER Benchmark)

CANONICAL RESULTS:
- Sufficiency: 100.0% ¬± 0.0%
- Comprehensiveness: 22.0% ¬± 17.3%
- F1 Score: 36.0%
- Improvement vs Attention: +75%
================================================================================
"""
```

---

### Notebook 06: Failure Analysis

**Header update:**
```python
"""
================================================================================
NOTEBOOK 06: FAILURE ANALYSIS (RQ3)
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Analyze when and why explanations diverge from mechanisms (RQ3).

Sample Size: n=50 prompts

HEURISTIC THRESHOLDS DISCLAIMER:
The comprehensiveness thresholds (15%, 25%) are HEURISTIC values chosen for 
interpretability. They are not derived from statistical analysis. Different 
thresholds may yield different categorizations.
================================================================================
"""
```

---

### Notebook 07: e-SNLI Format Study

**Header update:**
```python
"""
================================================================================
NOTEBOOK 07: E-SNLI FORMAT STUDY
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Analyze e-SNLI explanation formats to inform template design.
         This provides methodology justification for explanation structure.

Key Reference: Camburu et al., 2018 (e-SNLI Dataset)
================================================================================
"""
```

---

### Notebook 08: Learned NL Generator (NOVEL)

**Header update:**
```python
"""
================================================================================
NOTEBOOK 08: LEARNED NL GENERATOR (NOVEL CONTRIBUTION)
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

NOVEL CONTRIBUTION:
First systematic use of LLMs to generate natural language explanations from 
mechanistic circuit analysis.

Sample Size: n=20 prompts (API cost constraint)
LLM Used: Claude Sonnet (as language RENDERER, not explainer)

CLARIFICATION: 
Claude converts mechanistic data to natural language. It does NOT discover 
or verify the circuit. Ground truth comes from activation patching.

GitHub: PRIVATE (proprietary methodology)
================================================================================
"""
```

---

### Notebook 09: Template vs Learned

**Header update:**
```python
"""
================================================================================
NOTEBOOK 09: TEMPLATE VS LEARNED COMPARISON
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Quantitative comparison of template-based vs LLM-generated explanations.

Sample Size: n=30 prompts (API cost constraint)

KEY RESULTS:
- Template quality: 60%
- Learned quality: 98%
- Improvement: +63%

GitHub: PRIVATE (uses Claude API)
================================================================================
"""
```

---

### Notebook 10: Final Evaluation

**Header update:**
```python
"""
================================================================================
NOTEBOOK 10: FINAL EVALUATION & THESIS SUMMARY
================================================================================
MSc Thesis: Explainable AI for LLMs via Mechanistic Interpretability
Author: Ajay Mahale
Supervisor: Prof. Dr. Haffner | Hochschule Trier

Purpose: Comprehensive summary loading all experimental results.

CANONICAL RESULTS SUMMARY:
- IOI Circuit: 61.4% coverage (6 heads)
- ERASER: Suff 100%, Comp 22%, F1 36%
- Beat attention baseline by +75%
- Learned explanations +63% vs templates
- 34% of cases show distributed computation

KEY LIMITATIONS:
- GPT-2 Small only
- IOI task only
- Results may not generalize
================================================================================
"""
```

---

## üìã CHECKLIST

For each notebook, verify:

| Notebook | Seed Set | Versions Logged | Header Updated | Sample Size Stated |
|----------|----------|-----------------|----------------|-------------------|
| 01 | ‚¨ú | ‚¨ú | ‚¨ú | N/A |
| 02 | ‚¨ú | ‚¨ú | ‚¨ú | ‚¨ú n=50 |
| 03 | ‚¨ú | ‚¨ú | ‚¨ú | N/A |
| 04 | ‚¨ú | ‚¨ú | ‚¨ú | ‚¨ú n=5 (preliminary) |
| 05 | ‚¨ú | ‚¨ú | ‚¨ú | ‚¨ú n=50 (CANONICAL) |
| 06 | ‚¨ú | ‚¨ú | ‚¨ú | ‚¨ú n=50 |
| 07 | ‚¨ú | ‚¨ú | ‚¨ú | ‚¨ú |
| 08 | ‚¨ú | ‚¨ú | ‚¨ú | ‚¨ú n=20 |
| 09 | ‚¨ú | ‚¨ú | ‚¨ú | ‚¨ú n=30 |
| 10 | ‚¨ú | ‚¨ú | ‚¨ú | ‚¨ú |
